{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naren221/NLP/blob/main/NarendraMDS202336_Assignment02_Version_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNDNEeKPBG3d"
      },
      "source": [
        "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      Assignment 02\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtwSDRBeBjH7"
      },
      "source": [
        "## **Narendra. C** <br> **MDS202336**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE01cn8bB7bN"
      },
      "source": [
        "## 1. Implementing Modified Verison of COALS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d-5bqYaCMMG"
      },
      "source": [
        "* The preprocessed corpus is stored in JSON files, where each file contains a collection of documents.\n",
        "* Each document in the JSON file is represented as a key-value pair:\n",
        "\n",
        "    * The key is the document name or ID, which serves as a unique identifier for the document.\n",
        "    * The value is the preprocessed text of that document, which consists of cleaned and tokenized words from the original text (e.g., lowercase, punctuation removed, stopwords removed).\n",
        "\n",
        "* This structure allows easy access to both the document identifiers and their\n",
        "corresponding preprocessed content, facilitating efficient data manipulation and analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4DEBdK2FDAF",
        "outputId": "e9725a17-68eb-44c5-cbea-9ea4c6e73085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_yUyyvD5nAY"
      },
      "source": [
        "### Steps to Create a Co-occurrence Matrix\n",
        "\n",
        "1. **Identify Data Sources**\n",
        "   - Locate and list all relevant JSON files containing text data.\n",
        "\n",
        "2. **Extract Vocabulary**\n",
        "   - **Read Files:**\n",
        "     - Read and parse text data from each JSON file.\n",
        "   - **Build Vocabulary:**\n",
        "     - Extract unique tokens from each file and aggregate them into a global vocabulary.\n",
        "   - **Save Vocabulary:**\n",
        "     - Save the vocabulary to a file for future reference.\n",
        "\n",
        "3. **Initialize Co-occurrence Matrix**\n",
        "   - **Prepare Matrix:**\n",
        "     - Initialize a sparse matrix for storing co-occurrence counts based on the global vocabulary.\n",
        "\n",
        "4. **Process Each File**\n",
        "   - **Load Data:**\n",
        "     - Read and parse text data from each JSON file.\n",
        "   - **Update Matrix:**\n",
        "     - Convert tokens to matrix indices using the global vocabulary.\n",
        "     - Update the co-occurrence matrix based on token co-occurrences within a defined window size.\n",
        "\n",
        "5. **Finalize Matrix**\n",
        "   - **Ensure Symmetry:**\n",
        "     - Make the matrix symmetric by adding its transpose.\n",
        "   - **Convert Format:**\n",
        "     - Change the matrix to a more efficient storage format for further use (e.g., CSR format).\n",
        "\n",
        "6. **Save Results**\n",
        "   - **Store Matrix:**\n",
        "     - Save the matrix and vocabulary in a storage format for future use (e.g., HDF5 format).\n",
        "\n",
        "7. **Clean Up**\n",
        "   - Free up memory by deleting temporary data structures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apz3U2qF18cs",
        "outputId": "9f9157d6-2d64-45a1-e08f-236c257bbd16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting vocabulary: 100%|██████████| 12/12 [01:17<00:00,  6.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary extracted and saved to: /content/drive/MyDrive/vocabulary.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_vocabulary(directory, output_path='/content/drive/MyDrive/vocabulary.txt'):\n",
        "    # Get the list of JSON files in the directory\n",
        "    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
        "\n",
        "    if not json_files:\n",
        "        raise ValueError(\"No JSON files found in the directory.\")\n",
        "\n",
        "    # Initialize a set to store the vocabulary\n",
        "    vocab = set()\n",
        "\n",
        "    # Process each JSON file with a progress bar\n",
        "    for json_file_name in tqdm(json_files, desc='Extracting vocabulary'):\n",
        "        file_path = os.path.join(directory, json_file_name)\n",
        "\n",
        "        # Read the JSON file\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Collect unique tokens from the current file\n",
        "        for document in data:\n",
        "            vocab.update(document)\n",
        "\n",
        "        # Free the data from memory\n",
        "        del data\n",
        "\n",
        "    # Save the vocabulary to a file\n",
        "    with open(output_path, 'w') as file:\n",
        "        for word in sorted(vocab):\n",
        "            file.write(f\"{word}\\n\")\n",
        "\n",
        "    print(f\"Vocabulary extracted and saved to: {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "directory = '/content/drive/MyDrive/NLP Assignments/preprocessed_files/'\n",
        "output_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "extract_vocabulary(directory, output_path=output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load vocabulary\n",
        "def load_vocabulary(vocab_path):\n",
        "    with open(vocab_path, 'r') as file:\n",
        "        vocab = [line.strip() for line in file]\n",
        "    return vocab\n",
        "\n",
        "# Function to process a single JSON file and build its co-occurrence matrix\n",
        "def process_single_file(file_path, vocab_index, window_size, output_path):\n",
        "    vocab_size = len(vocab_index)\n",
        "    cooccurrence_matrix = sp.lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
        "\n",
        "    # Read the JSON file\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Process each document in the current JSON file with a progress bar\n",
        "    with tqdm(total=len(data), desc=f'Processing {os.path.basename(file_path)}') as pbar:\n",
        "        for document in data:\n",
        "            token_indices = [vocab_index[token] for token in document if token in vocab_index]\n",
        "\n",
        "            for i, token_idx in enumerate(token_indices):\n",
        "                start_index = max(0, i - window_size)\n",
        "                end_index = min(len(token_indices), i + window_size + 1)\n",
        "\n",
        "                # Update co-occurrence for previous and future tokens without weights\n",
        "                for j in range(start_index, i):\n",
        "                    cooccurrence_matrix[token_idx, token_indices[j]] += 1\n",
        "\n",
        "                for j in range(i + 1, end_index):\n",
        "                    cooccurrence_matrix[token_idx, token_indices[j]] += 1\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Convert to CSR format for efficient storage\n",
        "    cooccurrence_matrix_csr = cooccurrence_matrix.tocsr()\n",
        "\n",
        "    # Store the matrix in a file\n",
        "    output_file = os.path.join(output_path, f'cooccurrence_matrix_{os.path.basename(file_path).split(\".\")[0]}.h5')\n",
        "    with h5py.File(output_file, 'w') as f:\n",
        "        f.create_dataset('data', data=cooccurrence_matrix_csr.data, compression='gzip')\n",
        "        f.create_dataset('indices', data=cooccurrence_matrix_csr.indices, compression='gzip')\n",
        "        f.create_dataset('indptr', data=cooccurrence_matrix_csr.indptr, compression='gzip')\n",
        "        f.attrs['shape'] = cooccurrence_matrix_csr.shape\n",
        "\n",
        "    # Free memory\n",
        "    del cooccurrence_matrix\n",
        "    del cooccurrence_matrix_csr\n",
        "    del data\n",
        "\n",
        "    print(f\"Co-occurrence matrix for {file_path} stored at: {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "def process_json_file(json_file_path, vocab_path, output_path, window_size=4):\n",
        "    vocab = load_vocabulary(vocab_path)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "    # Process and store co-occurrence matrix for the specified JSON file\n",
        "    process_single_file(json_file_path, vocab_index, window_size, output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__nrbo7tg_X_",
        "outputId": "991529d7-19b3-45a4-9836-bd777b11c61d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE5kFZUE2wmE",
        "outputId": "9d5c6507-eb0b-48b9-b821-2223d9a871d2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunk_5.json: 100%|██████████| 4711/4711 [23:00<00:00,  3.41it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_5.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_5.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunk_6.json: 100%|██████████| 4711/4711 [23:07<00:00,  3.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_6.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_6.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_7.json: 100%|██████████| 4711/4711 [22:10<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_7.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_7.h5\n"
          ]
        }
      ],
      "source": [
        "# file_number =  3 # Replace with the desired file number\n",
        "\n",
        "for file_number in [5, 6, 7]:\n",
        "  json_file_path = f'/content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_{file_number}.json'  # Path to your specific JSON file\n",
        "  directory = '/content/drive/MyDrive/NLP Assignments/preprocessed_files/'\n",
        "  vocab_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "  output_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/'\n",
        "\n",
        "  process_json_file(json_file_path, vocab_path, output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file_number in [8, 9, 10]:\n",
        "  json_file_path = f'/content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_{file_number}.json'  # Path to your specific JSON file\n",
        "  directory = '/content/drive/MyDrive/NLP Assignments/preprocessed_files/'\n",
        "  vocab_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "  output_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/'\n",
        "\n",
        "  process_json_file(json_file_path, vocab_path, output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY9iGrraKH8n",
        "outputId": "84a6323d-87fe-4189-980c-d6aa9a4cf22c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_8.json: 100%|██████████| 4711/4711 [21:41<00:00,  3.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_8.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_8.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_9.json: 100%|██████████| 4711/4711 [23:04<00:00,  3.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_9.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_9.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_10.json: 100%|██████████| 4711/4711 [24:27<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_10.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_10.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file_number in [11,12]:\n",
        "  json_file_path = f'/content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_{file_number}.json'  # Path to your specific JSON file\n",
        "  directory = '/content/drive/MyDrive/NLP Assignments/preprocessed_files/'\n",
        "  vocab_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "  output_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/'\n",
        "\n",
        "  process_json_file(json_file_path, vocab_path, output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neLUEkfc66h3",
        "outputId": "cecefd98-825f-4e0f-89fe-44825a9d3b41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_11.json: 100%|██████████| 4711/4711 [15:33<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_11.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_11.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_12.json: 100%|██████████| 4707/4707 [14:34<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_12.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_12.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I built a cooccurrence matrix for each chunk of the corpus.\n",
        "* This was done to manage tasks with limited memory.\n",
        "* Now i will combine these matrices to form a single cooccurrence matrix for the entire corpus."
      ],
      "metadata": {
        "id": "FbZWQpiXqP1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy.sparse as sp\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to incrementally combine matrices and write them to disk\n",
        "def combine_cooccurrence_matrices_incrementally(folder_path, vocab_size, output_file):\n",
        "    final_matrix = None\n",
        "\n",
        "    # Get all .h5 files in the folder\n",
        "    matrix_files = [f for f in os.listdir(folder_path) if f.startswith('cooccurrence_matrix') and f.endswith('.h5')]\n",
        "    if not matrix_files:\n",
        "        raise ValueError(\"No co-occurrence matrix files found in the folder.\")\n",
        "\n",
        "    # Combine each matrix into the final matrix incrementally\n",
        "    for file in tqdm(matrix_files, desc=\"Combining matrices\"):\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            data = f['data'][:]\n",
        "            indices = f['indices'][:]\n",
        "            indptr = f['indptr'][:]\n",
        "            shape = f.attrs['shape']\n",
        "\n",
        "        cooccurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "        # If final_matrix is None (i.e., first matrix), set it. Otherwise, add the new matrix\n",
        "        if final_matrix is None:\n",
        "            final_matrix = cooccurrence_matrix\n",
        "        else:\n",
        "            final_matrix = final_matrix + cooccurrence_matrix\n",
        "\n",
        "        # After processing each matrix, save intermediate results to avoid memory overload\n",
        "        _save_intermediate(final_matrix, output_file)\n",
        "        print(f\"\\nProcessed {file} and saved intermediate result.\")\n",
        "\n",
        "    # Return the final matrix\n",
        "    return final_matrix\n",
        "\n",
        "# Helper function to save intermediate results\n",
        "def _save_intermediate(matrix, output_file):\n",
        "    with h5py.File(output_file, 'w') as f:\n",
        "        f.create_dataset('data', data=matrix.data, compression='gzip')\n",
        "        f.create_dataset('indices', data=matrix.indices, compression='gzip')\n",
        "        f.create_dataset('indptr', data=matrix.indptr, compression='gzip')\n",
        "        f.attrs['shape'] = matrix.shape\n",
        "\n",
        "    print(f\"Intermediate result saved to: {output_file}\")\n",
        "\n",
        "# Function to store the final combined matrix\n",
        "def store_final_matrix(final_matrix, folder_path):\n",
        "    final_output_path = os.path.join(folder_path, 'final_cooccurrence_matrix.h5')\n",
        "    _save_intermediate(final_matrix, final_output_path)\n",
        "    print(f\"Final co-occurrence matrix stored at: {final_output_path} 🎉\")\n"
      ],
      "metadata": {
        "id": "xNLr-FBGynH-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices'  # Folder where matrices are hanging out\n",
        "vocab_size = len(load_vocabulary('/content/drive/MyDrive/NLP Assignments/vocabulary.txt'))  # Load your vocab size\n",
        "output_file = '/content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5'  # Path to store the intermediate/final matrix\n",
        "\n",
        "# Call the function to combine matrices incrementally\n",
        "final_matrix = combine_cooccurrence_matrices_incrementally(folder_path, vocab_size, output_file)\n",
        "\n",
        "# After processing, you can store the final result\n",
        "store_final_matrix(final_matrix, '/content/drive/MyDrive/NLP Assignments/Assignment_02')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dvH6J1R1fnu",
        "outputId": "16313871-ada0-41b9-f0b2-e49e1884fa5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Combining matrices:   8%|▊         | 1/12 [00:12<02:12, 12.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_3.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  17%|█▋        | 2/12 [00:31<02:41, 16.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_1.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  25%|██▌       | 3/12 [00:56<03:03, 20.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_2.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  33%|███▎      | 4/12 [01:26<03:12, 24.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_4.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  42%|████▏     | 5/12 [02:02<03:20, 28.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_5.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  50%|█████     | 6/12 [02:44<03:17, 32.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_6.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  58%|█████▊    | 7/12 [03:28<03:03, 36.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_7.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  67%|██████▋   | 8/12 [04:16<02:41, 40.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_8.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  75%|███████▌  | 9/12 [05:10<02:13, 44.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_9.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  83%|████████▎ | 10/12 [06:09<01:38, 49.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_10.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  92%|█████████▏| 11/12 [07:11<00:52, 52.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_11.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Combining matrices: 100%|██████████| 12/12 [08:14<00:00, 41.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_12.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/final_cooccurrence_matrix.h5\n",
            "Final co-occurrence matrix stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/final_cooccurrence_matrix.h5 🎉\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "def view_submatrix(file_path, row_start, col_start, size=5):\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        data = f['data'][:]\n",
        "        indices = f['indices'][:]\n",
        "        indptr = f['indptr'][:]\n",
        "        shape = f.attrs['shape']\n",
        "\n",
        "        # Create the CSR matrix from the stored data\n",
        "        cooccurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "        # Extract the submatrix\n",
        "        submatrix = cooccurrence_matrix[row_start:row_start + size, col_start:col_start + size].toarray()\n",
        "\n",
        "        print(\"10x10 Submatrix:\")\n",
        "        print(submatrix)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/final_cooccurrence_matrix.h5'\n",
        "view_submatrix(file_path, row_start=0, col_start=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMdnG_SnHsZR",
        "outputId": "f36a4542-b859-4233-e74b-2fe74cff39cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10x10 Submatrix:\n",
            "[[4.116e+03 4.900e+01 4.000e+00 1.000e+00 0.000e+00]\n",
            " [4.900e+01 1.600e+02 5.000e+00 2.000e+00 0.000e+00]\n",
            " [4.000e+00 5.000e+00 3.400e+01 1.000e+00 1.000e+00]\n",
            " [1.000e+00 2.000e+00 1.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "vocab = load_vocabulary(vocab_path)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLCxnbmVgmY1",
        "outputId": "a801e637-1857-48dd-a75c-126a1e77f3c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1470595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYWk3GJVmF6p",
        "outputId": "151a6c7c-8113-4949-8373-9545b7095efb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now i will try to reduce the size of the vocabulary to 7000."
      ],
      "metadata": {
        "id": "liWp8cHGnnQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I am reducing the vocabulary size by **filtering out infrequent words**, specifically those that have **low co-occurrence frequencies**. Additionally, stop words will be removed. This optimization aims to enhance the co-occurrence matrix and improve overall efficiency.\n"
      ],
      "metadata": {
        "id": "01Z3tBjfwp6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reason for Reducing Vocabulary Size This Way\n",
        "\n",
        "* Reducing vocabulary size based on **co-occurrence frequency** helps prioritize words that frequently appear together, capturing more meaningful word relationships.\n",
        "* Filtering out terms with low co-occurrence reduces noise and leads to a more efficient, less sparse co-occurrence matrix, improving computational performance.\n",
        "* This method emphasizes relationships between words rather than individual token frequency, offering a more insightful analysis of the text.\n",
        "* The approach focuses on selecting the **top 7000** words based on co-occurrence frequency, ensuring that significant terms are retained while less informative ones are removed.\n"
      ],
      "metadata": {
        "id": "56WFQalZkoDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import h5py\n",
        "\n",
        "# Step 1: Load the original co-occurrence matrix from the HDF5 file\n",
        "def load_cooccurrence_matrix(hdf5_file):\n",
        "    with h5py.File(hdf5_file, 'r') as f:\n",
        "        data = f['data'][:]\n",
        "        indices = f['indices'][:]\n",
        "        indptr = f['indptr'][:]\n",
        "        shape = f.attrs['shape']\n",
        "\n",
        "        # Reconstruct sparse matrix\n",
        "        cooccurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "    return cooccurrence_matrix\n",
        "\n",
        "# Step 2: Load the original vocabulary (for 1.4 million words)\n",
        "def load_vocabulary(vocab_file):\n",
        "    with open(vocab_file, 'r') as file:\n",
        "        vocab = [line.strip() for line in file]\n",
        "    return vocab\n",
        "\n",
        "# Step 3: Sum co-occurrence frequencies for each word\n",
        "def get_cooccurrence_sums(cooccurrence_matrix):\n",
        "    # Sum along rows to get the total co-occurrence frequency for each word\n",
        "    word_cooccurrence_sums = np.array(cooccurrence_matrix.sum(axis=1)).flatten()\n",
        "    return word_cooccurrence_sums\n",
        "\n",
        "# Step 4: Rank words by total co-occurrence frequency and get top N words\n",
        "def select_top_words(cooccurrence_sums, vocab_size):\n",
        "    # Get indices of top N words based on co-occurrence sums\n",
        "    top_word_indices = np.argsort(cooccurrence_sums)[-vocab_size:]\n",
        "    return top_word_indices\n",
        "\n",
        "# Step 5: Filter the co-occurrence matrix to keep only the top words\n",
        "def filter_cooccurrence_matrix(cooccurrence_matrix, top_word_indices):\n",
        "    # Extract rows and columns corresponding to top_word_indices\n",
        "    reduced_matrix = cooccurrence_matrix[top_word_indices, :][:, top_word_indices]\n",
        "    return reduced_matrix\n",
        "\n",
        "# Step 6: Save the reduced co-occurrence matrix and reduced vocabulary\n",
        "def save_reduced_matrix_and_vocab(reduced_matrix, reduced_vocab, output_matrix_file, output_vocab_file):\n",
        "    # Save reduced co-occurrence matrix to HDF5\n",
        "    with h5py.File(output_matrix_file, 'w') as f:\n",
        "        f.create_dataset('data', data=reduced_matrix.data, compression='gzip')\n",
        "        f.create_dataset('indices', data=reduced_matrix.indices, compression='gzip')\n",
        "        f.create_dataset('indptr', data=reduced_matrix.indptr, compression='gzip')\n",
        "        f.attrs['shape'] = reduced_matrix.shape\n",
        "\n",
        "    # Save reduced vocabulary to a text file\n",
        "    with open(output_vocab_file, 'w') as f:\n",
        "        for word in reduced_vocab:\n",
        "            f.write(f\"{word}\\n\")\n",
        "\n",
        "# Main function to reduce co-occurrence matrix and vocabulary by co-occurrence frequency\n",
        "def reduce_matrix_and_vocab(hdf5_file, vocab_file, output_matrix_file, output_vocab_file, vocab_size):\n",
        "    # Load the original co-occurrence matrix\n",
        "    cooccurrence_matrix = load_cooccurrence_matrix(hdf5_file)\n",
        "\n",
        "    # Load the original vocabulary\n",
        "    vocab = load_vocabulary(vocab_file)\n",
        "\n",
        "    # Get co-occurrence sums for each word\n",
        "    cooccurrence_sums = get_cooccurrence_sums(cooccurrence_matrix)\n",
        "\n",
        "    # Select top words based on co-occurrence frequency\n",
        "    top_word_indices = select_top_words(cooccurrence_sums, vocab_size)\n",
        "\n",
        "    # Filter the matrix to keep only the top words\n",
        "    reduced_matrix = filter_cooccurrence_matrix(cooccurrence_matrix, top_word_indices)\n",
        "\n",
        "    # Get the reduced vocabulary\n",
        "    reduced_vocab = [vocab[i] for i in top_word_indices]\n",
        "\n",
        "    # Save the reduced matrix and vocabulary\n",
        "    save_reduced_matrix_and_vocab(reduced_matrix, reduced_vocab, output_matrix_file, output_vocab_file)\n",
        "\n",
        "# Paths to original matrix, vocab, and reduced output files\n",
        "original_matrix_file = '/content/drive/MyDrive/NLP Assignments/Assignment_02/final_cooccurrence_matrix.h5'\n",
        "original_vocab_file = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "reduced_matrix_file = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_cooccurrence_matrix.h5'\n",
        "reduced_vocab_file = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_vocabulary_2.txt'\n",
        "vocab_size = 7000  # Number of top words to keep\n",
        "\n",
        "# Run the reduction process\n",
        "reduce_matrix_and_vocab(original_matrix_file, original_vocab_file, reduced_matrix_file, reduced_vocab_file, vocab_size)\n"
      ],
      "metadata": {
        "id": "dVhmxD8TLk7W"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_vocabulary_2.txt'\n",
        "vocab = load_vocabulary(vocab_path)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnOoTZhOW_gt",
        "outputId": "da023834-8bce-44f1-943c-63f2b20af85b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. I noticed some letters present in the vocab which can be considered as stop words ... for eg there are alphabets like `e` and all which are just letters. I will remove those from the vocabulary."
      ],
      "metadata": {
        "id": "_UrxRH3gW5EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of stop words\n",
        "stop_words = [\n",
        "    'given', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n",
        "    'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
        "    'u', 'v', 'w', 'x', 'y', 'z'\n",
        "]\n",
        "\n",
        "# Removing stop words from the vocabulary\n",
        "filtered_vocab = [word for word in vocab if word not in stop_words]\n",
        "\n",
        "print(len(filtered_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9vCULX-W6Rf",
        "outputId": "622be622-0f7b-4adf-faa3-5a55e2d0e57b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Since the corpus is made from research papers things like `et.al.` will be present. we can remove those as well."
      ],
      "metadata": {
        "id": "STGZ9mPtXQnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of terms to remove\n",
        "terms_to_remove = ['et', 'al', 'ie', 'eg', 'etc', 'aka', 'viz', 'cf', 'nb', 'pm', 'vs']\n",
        "\n",
        "# Remove the terms from the vocabulary\n",
        "filtered_vocab = [word for word in filtered_vocab if word not in terms_to_remove]\n",
        "\n",
        "len(filtered_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T0FdFHQXPhD",
        "outputId": "203dbb40-b9ac-4811-e72c-252110d216ad"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6973"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now i will store this filtered vocablary."
      ],
      "metadata": {
        "id": "BfDq7JLWXeIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the filtered vocabulary\n",
        "filtered_vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_vocab_2.txt'\n",
        "with open(filtered_vocab_path, 'w') as f:\n",
        "    f.write(\"\\n\".join(filtered_vocab))"
      ],
      "metadata": {
        "id": "PrSsBW_OXgS1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now i will do corresponding changes to the cooccurrence matrix as well."
      ],
      "metadata": {
        "id": "vW2-4tdAXwoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the original co-occurrence matrix (sparse format)\n",
        "co_occurrence_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_cooccurrence_matrix.h5'\n",
        "with h5py.File(co_occurrence_path, 'r') as f:\n",
        "    data = f['data'][:]\n",
        "    indices = f['indices'][:]\n",
        "    indptr = f['indptr'][:]\n",
        "    shape = f.attrs['shape']\n",
        "\n",
        "# Rebuild the sparse co-occurrence matrix (CSR format)\n",
        "co_occurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "# Load the filtered vocabulary\n",
        "filtered_vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_vocab_2.txt'\n",
        "with open(filtered_vocab_path, 'r') as file:\n",
        "    filtered_vocab = [line.strip() for line in file]\n",
        "\n",
        "# Create a mapping from vocabulary words to their indices\n",
        "vocab_index = {word: i for i, word in enumerate(filtered_vocab)}\n",
        "\n",
        "# Create a new co-occurrence matrix based on the filtered vocabulary\n",
        "filtered_matrix = sp.lil_matrix((len(filtered_vocab), len(filtered_vocab)), dtype=np.float32)\n",
        "\n",
        "# Populate the filtered co-occurrence matrix based on the original matrix with progress bar\n",
        "for word, new_index in tqdm(vocab_index.items(), desc=\"Building filtered co-occurrence matrix\", total=len(vocab_index)):\n",
        "    original_index = vocab_index.get(word)  # Get the index of the word in the original matrix\n",
        "\n",
        "    if original_index is not None:\n",
        "        # Copy relevant row and column from the original matrix\n",
        "        filtered_matrix[new_index, :] = co_occurrence_matrix[original_index, :].toarray()[0, list(vocab_index.values())]\n",
        "        filtered_matrix[:, new_index] = co_occurrence_matrix[:, original_index].toarray()[list(vocab_index.values()), 0]\n",
        "\n",
        "# Convert the matrix to CSR for efficiency\n",
        "filtered_matrix_csr = filtered_matrix.tocsr()\n",
        "\n",
        "# Save the new filtered co-occurrence matrix\n",
        "filtered_co_occurrence_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_cooccurrence_matrix_2.h5'\n",
        "with h5py.File(filtered_co_occurrence_path, 'w') as f:\n",
        "    f.create_dataset('data', data=filtered_matrix_csr.data, compression='gzip')\n",
        "    f.create_dataset('indices', data=filtered_matrix_csr.indices, compression='gzip')\n",
        "    f.create_dataset('indptr', data=filtered_matrix_csr.indptr, compression='gzip')\n",
        "    f.attrs['shape'] = filtered_matrix_csr.shape\n",
        "\n",
        "print(f\"Filtered co-occurrence matrix saved at: {filtered_co_occurrence_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpxnTsIqXvR0",
        "outputId": "8a24b180-aec0-4d16-da0e-eb189905e7fa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building filtered co-occurrence matrix: 100%|██████████| 6973/6973 [21:16<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered co-occurrence matrix saved at: /content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_cooccurrence_matrix_2.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Co-occurrence Matrix\n",
        "\n",
        "The co-occurrence matrix used in this assignment is too large to be displayed here directly. You can download or view the matrix using the following link:\n",
        "\n",
        "[Download Co-occurrence Matrix](https://drive.google.com/file/d/1-1Ga3x4Pl0GexVOtseEdYzXhSKccLWWb/view?usp=drive_link)\n",
        "\n",
        "_Note: This matrix is stored in HDF5 format due to its large size._\n"
      ],
      "metadata": {
        "id": "nyfznYHH0dZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "def view_submatrix(file_path, row_start, col_start, size=5):\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        data = f['data'][:]\n",
        "        indices = f['indices'][:]\n",
        "        indptr = f['indptr'][:]\n",
        "        shape = f.attrs['shape']\n",
        "\n",
        "        # Create the CSR matrix from the stored data\n",
        "        cooccurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "        # Extract the submatrix\n",
        "        submatrix = cooccurrence_matrix[row_start:row_start + size, col_start:col_start + size].toarray()\n",
        "\n",
        "        print(\"10x10 Submatrix:\")\n",
        "        print(submatrix)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_cooccurrence_matrix_2.h5'\n",
        "view_submatrix(file_path, row_start=0, col_start=0, size = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iV966NnfWa7",
        "outputId": "52fd6f28-eee8-449b-d2d6-63c38aaac38f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10x10 Submatrix:\n",
            "[[  2.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0. 170.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.  84.   0.   0.   0.   1.   1.   0.   0.]\n",
            " [  0.   0.   0. 246.   0.   0.   2.   0.   3.  29.]\n",
            " [  0.   0.   0.   0.  74.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 220.   0.   0.   0.   0.]\n",
            " [  0.   0.   1.   2.   0.   0.  16.   0.   0.   0.]\n",
            " [  0.   0.   1.   0.   0.   0.   0.  38.   0.   0.]\n",
            " [  0.   0.   0.   3.   0.   0.   0.   0.  62.  15.]\n",
            " [  0.   0.   0.  29.   0.   0.   0.   0.  15.  54.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing the entries using proabilities $\\frac {P_{ik}} {P_{jk}}$\n",
        "\n",
        "$X$      -> Co-occurrence Matrix      <br>\n",
        "$X_{ij}$ -> Number of times word j occurs in the context of word i. <br>\n",
        "$X_i = \\sum_k X_{ik}$ -> Number of any word appears in the context of word i.<br>\n",
        "$P_{ij} = P(j|i) = \\frac {X_{ij} } {X_i}$ <br>\n",
        "This gives the probability that the word j occurs in the contex of the word i.\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\frac {P_{ik}} {P_{jk}} = {P(k|i)} {P(k|j)} $ <br>\n",
        "\n",
        "<br>\n",
        "The ratio  $ \\frac {P(k|i)} {P(k|j)}$\n",
        "helps normalize co-occurrence counts by considering the probability of context words with respect to different target words. This normalization emphasizes the relative strength of association between target words, making it easier to distinguish meaningful patterns and relationships.\n",
        "\n",
        "### Choosing the word for k\n",
        "\n",
        "**Value of k:** High-Frequency Words (e.g., \"the\", \"and\", \"of\")\n",
        "\n",
        "**Reason for Choosing This:**\n",
        "\n",
        "In co-occurrence matrix normalization, selecting high-frequency words as the reference `k` helps address the issue of frequency bias. High-frequency words, being common across various contexts, serve as a stable baseline for comparison. By normalizing co-occurrence counts using these high-frequency words, we adjust for their overwhelming presence and mitigate their disproportionate influence. The ratio of co-occurrence probabilities $ \\frac{P(k | i)}{P(k | j)} $ helps us understand the relative strength of the association between the target words `i` and `j` compared to their association with a high-frequency word `k`. This normalization process emphasizes the distinctive relationships between `i` and `j` by neutralizing the impact of common words that might otherwise dominate the matrix. Thus, using high-frequency words ensures that the co-occurrence matrix reflects more meaningful and specific patterns rather than being skewed by the frequent occurrence of generic words.\n",
        "\n"
      ],
      "metadata": {
        "id": "5U_eC53NyACN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps for Normalizing Co-occurrence Matrix\n",
        "\n",
        "1. **Find the Most Frequent Word**:\n",
        "   Start by summing each row (or column) of the co-occurrence matrix to get the total occurrences of each word across all contexts. The word with the highest total is identified as the most frequent word.\n",
        "\n",
        "2. **Normalize the Co-occurrence Matrix**:\n",
        "   Using the most frequent word $ k $, normalize the co-occurrence matrix. The normalization is done by calculating the ratio $ \\frac{P(k | i)}{P(k | j)} $, where:\n",
        "   - $ P(k | i) $ is the probability of word $ k $ occurring in the context of word $ i $.\n",
        "   - $ P(k | j) $ is the probability of word $ k $ occurring in the context of word $ j $.\n",
        "   - This step adjusts the co-occurrence counts by considering the probability of context words with respect to different target words.\n",
        "\n",
        "3. **Remove the Most Frequent Word**:\n",
        "   Once the normalization is done, remove the row and column corresponding to the most frequent word from the co-occurrence matrix. Also, remove the most frequent word from the vocabulary list, so it no longer influences further analysis.\n"
      ],
      "metadata": {
        "id": "KNy6AbuYzMMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1. Finding the most Frequent word."
      ],
      "metadata": {
        "id": "tM38GykWhjSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "def load_cooccurrence_matrix(hdf5_file):\n",
        "    with h5py.File(hdf5_file, 'r') as f:\n",
        "        data = f['data'][:]\n",
        "        indices = f['indices'][:]\n",
        "        indptr = f['indptr'][:]\n",
        "        shape = f.attrs['shape']\n",
        "\n",
        "        # Reconstruct sparse matrix\n",
        "        cooccurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "    return cooccurrence_matrix\n",
        "\n",
        "def load_vocabulary(vocab_path):\n",
        "    with open(vocab_path, 'r') as f:\n",
        "        vocabulary = [line.strip() for line in f.readlines()]\n",
        "    return vocabulary\n",
        "\n",
        "def find_most_frequent_word(cooccurrence_matrix, vocabulary):\n",
        "    word_frequencies = np.sum(cooccurrence_matrix, axis=1)\n",
        "    most_frequent_word_index = np.argmax(word_frequencies)\n",
        "    return most_frequent_word_index, vocabulary[most_frequent_word_index]\n"
      ],
      "metadata": {
        "id": "Ip7WK3mkjRhF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hdf5_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_cooccurrence_matrix_2.h5'\n",
        "vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_vocab_2.txt'\n",
        "\n",
        "X = load_cooccurrence_matrix(hdf5_path)\n",
        "vocabulary = load_vocabulary(vocab_path)\n",
        "\n",
        "k, most_frequent_word = find_most_frequent_word(X, vocabulary)\n",
        "print(f\"Most Frequent Word (k): {most_frequent_word} (Index: {k})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYW0XIJr32K8",
        "outputId": "e0979e43-781b-4b5a-a90a-a1c4e36486c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Frequent Word (k): patients (Index: 6972)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq_vuzwS861g",
        "outputId": "49dd1ab9-1784-489e-b8af-1fc1a6b41b54"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6973, 6973)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Normalize the co-occurrence matrix"
      ],
      "metadata": {
        "id": "ObUNnxEqijht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def normalize_cooccurrence_matrix(X, k):\n",
        "    m = X.shape[0]\n",
        "    X_dense = X.toarray()  # Convert sparse matrix to dense for easy manipulation\n",
        "    normalized_matrix = np.zeros_like(X_dense, dtype=np.float32)\n",
        "\n",
        "    # Calculate the sum of each row only once\n",
        "    row_sums = np.sum(X_dense, axis=1)\n",
        "\n",
        "    # Extract probabilities for the k-th word for all rows\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        P_ik = np.where(row_sums > 0, X_dense[:, k] / row_sums, 0)  # Handle division by zero\n",
        "\n",
        "    # Loop through each row to fill the normalized matrix\n",
        "    for i in tqdm(range(m), desc=\"Processing X\", total=m):\n",
        "        # Extract probabilities for the current row\n",
        "        X_jk = X_dense[:, k]  # This will be the same for each row i\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            P_jk = np.where(row_sums > 0, X_jk / row_sums, 0)  # Handle division by zero\n",
        "\n",
        "        # Normalize for row i, but ensure that if X[i, j] is 0, the value stays 0\n",
        "        non_zero_mask = (X_dense[i, :] != 0)  # Mask to check where X[i, :] is non-zero\n",
        "        normalized_matrix[i, non_zero_mask] = P_ik[i] / P_jk[non_zero_mask]  # Normalize where non-zero\n",
        "        normalized_matrix[i, ~non_zero_mask] = 0  # Set to 0 where X[i, :] is zero\n",
        "\n",
        "    # Replace infinity values with zeros\n",
        "    normalized_matrix[np.isinf(normalized_matrix)] = 0.0\n",
        "    normalized_matrix[np.isnan(normalized_matrix)] = 0.0\n",
        "\n",
        "    # Convert back to sparse matrix\n",
        "    return normalized_matrix\n"
      ],
      "metadata": {
        "id": "7iCYkkKqSDHN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def normalize_cooccurrence_matrix(X, k):\n",
        "    m = X.shape[0]\n",
        "    X_dense = X.toarray()  # Convert sparse matrix to dense for easy manipulation\n",
        "    normalized_matrix = np.zeros_like(X_dense, dtype=np.float32)\n",
        "\n",
        "    # Calculate the sum of each row only once\n",
        "    row_sums = np.sum(X_dense, axis=1)\n",
        "\n",
        "    # Extract probabilities for the k-th word for all rows\n",
        "    # Avoid division by zero\n",
        "    P_ik = np.where(row_sums > 0, X_dense[:, k] / row_sums, 0)\n",
        "\n",
        "    # Loop through each row to fill the normalized matrix\n",
        "    for i in tqdm(range(m), desc=\"Processing X\", total=m):\n",
        "        # Extract probabilities for the current row\n",
        "        X_jk = X_dense[:, k]  # This will be the same for each row i\n",
        "\n",
        "        # Avoid division by zero for P_jk\n",
        "        P_jk = np.where(row_sums > 0, X_jk / row_sums, 0)\n",
        "\n",
        "        # Normalize for row i, but ensure that if X[i, j] is 0, the value stays 0\n",
        "        non_zero_mask = (X_dense[i, :] != 0)  # Mask to check where X[i, :] is non-zero\n",
        "\n",
        "        # Ensure normalization does not result in inf or NaN\n",
        "        normalized_values = np.zeros_like(normalized_matrix[i, :])\n",
        "        valid_division = P_jk[non_zero_mask] > 0  # Check if P_jk is greater than 0\n",
        "        normalized_values[non_zero_mask] = np.where(valid_division, P_ik[i] / P_jk[non_zero_mask], 0)\n",
        "\n",
        "        normalized_matrix[i, :] = normalized_values\n",
        "\n",
        "    return normalized_matrix\n"
      ],
      "metadata": {
        "id": "4-TRwGDtRDf5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCg9uxJPRHh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_normalized = normalize_cooccurrence_matrix(X, k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE-BJ5geSG-W",
        "outputId": "817b749d-e7d9-45cd-ae85-a2145c14e3c2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing X:   0%|          | 0/6973 [00:00<?, ?it/s]<ipython-input-37-3bf9ef77f150>:30: RuntimeWarning: divide by zero encountered in divide\n",
            "  normalized_values[non_zero_mask] = np.where(valid_division, P_ik[i] / P_jk[non_zero_mask], 0)\n",
            "<ipython-input-37-3bf9ef77f150>:30: RuntimeWarning: invalid value encountered in divide\n",
            "  normalized_values[non_zero_mask] = np.where(valid_division, P_ik[i] / P_jk[non_zero_mask], 0)\n",
            "Processing X: 100%|██████████| 6973/6973 [00:01<00:00, 3982.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def normalize_cooccurrence_matrix(X):\n",
        "    X_dense = X.toarray()  # Convert sparse matrix to dense for easy manipulation\n",
        "    m = X_dense.shape[0]\n",
        "    normalized_matrix = np.zeros_like(X_dense, dtype=np.float32)\n",
        "\n",
        "    # Calculate the sum of each row once\n",
        "    row_sums = np.sum(X_dense, axis=1, keepdims=True)  # Shape (m, 1)\n",
        "\n",
        "    # Compute P_ik for all rows (i) and for all columns (k)\n",
        "    P_ik = np.where(row_sums > 0, X_dense / row_sums, 0)  # Shape (m, n)\n",
        "\n",
        "    # Calculate the normalized matrix\n",
        "    for i in tqdm(range(m), desc=\"Processing X\", total=m):\n",
        "        P_jk = P_ik[i]  # Get P_jk for the current row i\n",
        "        normalized_matrix[i, :] = np.where(P_jk > 0, P_ik / P_jk, 0)  # Calculate P_ik/P_jk\n",
        "\n",
        "    # Replace infinity and NaN values with zeros\n",
        "    normalized_matrix[np.isinf(normalized_matrix)] = 0.0\n",
        "    normalized_matrix[np.isnan(normalized_matrix)] = 0.0\n",
        "\n",
        "    return normalized_matrix\n"
      ],
      "metadata": {
        "id": "Q-SglOhTLLjz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_normalized = normalize_cooccurrence_matrix(X, k)"
      ],
      "metadata": {
        "id": "KlmiaYUkLP0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Removing the most frequent word from the matrix and vocabulary\n"
      ],
      "metadata": {
        "id": "qHQYUjYeitBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_most_frequent_word(cooccurrence_matrix, most_frequent_word_index, vocabulary):\n",
        "    new_matrix = np.delete(cooccurrence_matrix, most_frequent_word_index, axis=0)\n",
        "    new_matrix = np.delete(new_matrix, most_frequent_word_index, axis=1)\n",
        "\n",
        "    new_vocabulary = [word for idx, word in enumerate(vocabulary) if idx != most_frequent_word_index]\n",
        "\n",
        "    return new_matrix, new_vocabulary"
      ],
      "metadata": {
        "id": "7tBrbqO8jRat"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_vocab_2.txt'\n",
        "\n",
        "vocabulary = load_vocabulary(vocab_path)\n",
        "\n",
        "X_final, final_vocab = remove_most_frequent_word(X_normalized, k, vocabulary)"
      ],
      "metadata": {
        "id": "UZYemT5PWSW1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10x10 Submatrix."
      ],
      "metadata": {
        "id": "mv6-uQOZXLN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_final[:10,:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1ZF7_8rWzAk",
        "outputId": "9a1e86bd-e4b9-455b-883a-f20f15c396a7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 1.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 1.        , 0.        , 0.        ,\n",
              "        0.        , 1.1039914 , 1.3774192 , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 1.        , 0.        ,\n",
              "        0.        , 0.27166787, 0.        , 1.1946853 , 1.2950433 ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 1.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        1.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.90580416, 3.6809652 , 0.        ,\n",
              "        0.        , 1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.72599536, 0.        , 0.        ,\n",
              "        0.        , 0.        , 1.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.8370405 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 1.        , 1.0840038 ],\n",
              "       [0.        , 0.        , 0.        , 0.7721749 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.922506  , 1.        ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I will save X_final and final_vocab."
      ],
      "metadata": {
        "id": "c3pQsviaEoGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_vocab_file = '/content/drive/MyDrive/NLP Assignments/Assignment_02/Final_Vocab.txt'\n",
        "with open(output_vocab_file, 'w') as f:\n",
        "  for word in final_vocab:\n",
        "    f.write(f\"{word}\\n\")"
      ],
      "metadata": {
        "id": "l9FhRu6MEyBn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/drive/MyDrive/NLP Assignments/Assignment_02/Final_Matrix.npy\",X_final)"
      ],
      "metadata": {
        "id": "AjwhrQxrHozm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying the Vocabulary size and the size of the matrix."
      ],
      "metadata": {
        "id": "TCRxpYNeXttJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vocabulary Size: {len(final_vocab)}\")\n",
        "print(f\"Matrix Shape: {X_final.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abH4YD7gX6aK",
        "outputId": "9693210e-ec16-4f34-c6c3-db119305f340"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 6972\n",
            "Matrix Shape: (6972, 6972)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec_a, vec_b):\n",
        "    # print(vec_a, vec_b)\n",
        "    dot_product = np.dot(vec_a, vec_b)\n",
        "    norm_a = np.linalg.norm(vec_a)\n",
        "    norm_b = np.linalg.norm(vec_b)\n",
        "    # print(dot_product)\n",
        "    # print(dot_product / (norm_a * norm_b) if norm_a > 0 and norm_b > 0 else 0)\n",
        "    return dot_product / (norm_a * norm_b) if norm_a > 0 and norm_b > 0 else 0\n",
        "\n",
        "def get_top_similar_words(cooccurrence_matrix, word_index, vocabulary, top_n=10):\n",
        "    # Get the vector for the specified word\n",
        "    target_vector = cooccurrence_matrix[word_index].flatten()\n",
        "\n",
        "    # Check if the target vector is non-zero\n",
        "    if np.count_nonzero(target_vector) == 0:\n",
        "        return []  # Return empty if the target vector is zero\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    similarities = []\n",
        "    for i in range(cooccurrence_matrix.shape[0]):\n",
        "        if i != word_index:  # Avoid comparing the word with itself\n",
        "            other_vector = cooccurrence_matrix[i].flatten()\n",
        "\n",
        "            # Check if the other vector is non-zero\n",
        "            if np.count_nonzero(other_vector) > 0:\n",
        "                similarity = cosine_similarity(target_vector, other_vector)\n",
        "                similarities.append((vocabulary[i], similarity))\n",
        "\n",
        "    # Sort by similarity score in descending order and get the top N\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_n]\n",
        "\n",
        "# Example usage\n",
        "vocab_path = \"/content/drive/MyDrive/NLP Assignments/Assignment_02/Final_Vocab.txt\"\n",
        "vocabulary = load_vocabulary(vocab_path)  # Your vocabulary list\n",
        "covid_index = vocabulary.index(\"covid\")  # Get the index of \"covid\"\n",
        "\n",
        "X_final = np.load(\"/content/drive/MyDrive/NLP Assignments/Assignment_02/Final_Matrix.npy\")\n",
        "top_similar_words = get_top_similar_words(X_final, covid_index, vocabulary, top_n = 40)\n",
        "\n",
        "print(\"Top 40 words similar to 'covid':\")\n",
        "for word, score in top_similar_words:\n",
        "    print(f\"{word}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mouL72RIYCNP",
        "outputId": "52129ef9-dc32-4ccb-d300-de91826a9c85"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 40 words similar to 'covid':\n",
            "treatment: 0.9732\n",
            "use: 0.9690\n",
            "studies: 0.9663\n",
            "proteins: 0.9617\n",
            "cases: 0.9590\n",
            "specific: 0.9590\n",
            "two: 0.9588\n",
            "disease: 0.9575\n",
            "one: 0.9575\n",
            "severe: 0.9553\n",
            "information: 0.9490\n",
            "activities: 0.9474\n",
            "immune: 0.9456\n",
            "data: 0.9455\n",
            "sample: 0.9429\n",
            "virus: 0.9409\n",
            "challenge: 0.9390\n",
            "response: 0.9389\n",
            "determined: 0.9385\n",
            "clinical: 0.9381\n",
            "increased: 0.9379\n",
            "license: 0.9375\n",
            "review: 0.9372\n",
            "considered: 0.9355\n",
            "cell: 0.9353\n",
            "per: 0.9351\n",
            "chest: 0.9344\n",
            "incubated: 0.9336\n",
            "de: 0.9334\n",
            "surveillance: 0.9326\n",
            "low: 0.9321\n",
            "stable: 0.9320\n",
            "role: 0.9319\n",
            "characteristics: 0.9297\n",
            "negative: 0.9292\n",
            "approach: 0.9290\n",
            "diagnostic: 0.9272\n",
            "free: 0.9263\n",
            "nature: 0.9252\n",
            "results: 0.9218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Five nouns and verbs relevant to COVID19 from the corpus.\n",
        "\n",
        "**Nouns:**\n",
        "\n",
        "- treatment\n",
        "- disease\n",
        "- virus\n",
        "- response\n",
        "- cases\n",
        "\n",
        "**Verbs:**\n",
        "\n",
        "- use\n",
        "- increased\n",
        "- reviewed\n",
        "- considered\n",
        "- incubated\n"
      ],
      "metadata": {
        "id": "TKfKJ6aTCsSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "Kz0A8aMzDKvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of nouns and verbs\n",
        "words_to_find = {\n",
        "    \"Nouns\": [\"treatment\", \"disease\", \"virus\", \"response\", \"cases\"],\n",
        "    \"Verbs\": [\"use\", \"increased\", \"reviewed\", \"considered\", \"incubated\"]\n",
        "}\n",
        "\n",
        "# Loop through each category and word\n",
        "for category, words in words_to_find.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    for word in words:\n",
        "        # Assume you have a way to get the index of each word in the vocabulary\n",
        "        word_index = vocabulary.index(word)  # Get the index of the word\n",
        "        top_similar_words = get_top_similar_words(X_final, word_index, vocabulary, top_n=5)\n",
        "\n",
        "        print(f\"\\nTop 5 words similar to '{word}':\")\n",
        "        for similar_word, score in top_similar_words:\n",
        "            print(f\"{similar_word}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Nt59bD9BC6",
        "outputId": "2bc10003-40aa-4eb7-e144-4afbc42fa860"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Nouns:\n",
            "\n",
            "Top 5 words similar to 'treatment':\n",
            "severe: 0.9762\n",
            "covid: 0.9732\n",
            "information: 0.9621\n",
            "review: 0.9581\n",
            "one: 0.9561\n",
            "\n",
            "Top 5 words similar to 'disease':\n",
            "specific: 0.9643\n",
            "role: 0.9628\n",
            "two: 0.9608\n",
            "covid: 0.9575\n",
            "diagnostic: 0.9500\n",
            "\n",
            "Top 5 words similar to 'virus':\n",
            "less: 0.9686\n",
            "findings: 0.9669\n",
            "cells: 0.9638\n",
            "expression: 0.9636\n",
            "protein: 0.9635\n",
            "\n",
            "Top 5 words similar to 'response':\n",
            "severe: 0.9451\n",
            "covid: 0.9389\n",
            "disease: 0.9377\n",
            "treatment: 0.9375\n",
            "nature: 0.9350\n",
            "\n",
            "Top 5 words similar to 'cases':\n",
            "role: 0.9740\n",
            "two: 0.9711\n",
            "covid: 0.9590\n",
            "virus: 0.9583\n",
            "pneumonia: 0.9545\n",
            "\n",
            "Verbs:\n",
            "\n",
            "Top 5 words similar to 'use':\n",
            "covid: 0.9690\n",
            "cell: 0.9607\n",
            "studies: 0.9566\n",
            "specific: 0.9515\n",
            "disease: 0.9478\n",
            "\n",
            "Top 5 words similar to 'increased':\n",
            "two: 0.9406\n",
            "cases: 0.9390\n",
            "covid: 0.9379\n",
            "activities: 0.9361\n",
            "disease: 0.9348\n",
            "\n",
            "Top 5 words similar to 'reviewed':\n",
            "notably: 0.7997\n",
            "lipid: 0.7916\n",
            "toward: 0.7887\n",
            "con: 0.7825\n",
            "detect: 0.7823\n",
            "\n",
            "Top 5 words similar to 'considered':\n",
            "role: 0.9548\n",
            "cases: 0.9540\n",
            "associated: 0.9525\n",
            "pneumonia: 0.9525\n",
            "receptor: 0.9486\n",
            "\n",
            "Top 5 words similar to 'incubated':\n",
            "treatment: 0.9540\n",
            "surveillance: 0.9528\n",
            "via: 0.9367\n",
            "severe: 0.9346\n",
            "chest: 0.9337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Replace NaN and infinity values with 0\n",
        "X_final_cleaned = np.nan_to_num(X_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# Get the rank of the cleaned matrix\n",
        "rank = np.linalg.matrix_rank(X_final_cleaned)\n",
        "\n",
        "print(f\"Rank of the matrix: {rank}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DkyEc9Zc6Fr",
        "outputId": "a93687b7-647d-4f34-9fd2-9511579f5f0c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank of the matrix: 3636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Count the number of NaN values\n",
        "num_nan = np.isnan(X_final).sum()\n",
        "\n",
        "# Count the number of positive and negative infinity values\n",
        "num_posinf = np.isposinf(X_final).sum()\n",
        "num_neginf = np.isneginf(X_final).sum()\n",
        "\n",
        "# Count the total number of infinity values (both positive and negative)\n",
        "num_inf = np.isinf(X_final).sum()\n",
        "\n",
        "# Output the counts\n",
        "print(f\"Number of NaN values: {num_nan}\")\n",
        "print(f\"Number of positive infinity values: {num_posinf}\")\n",
        "print(f\"Number of negative infinity values: {num_neginf}\")\n",
        "print(f\"Total number of infinity values: {num_inf}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDqgzY48MrtY",
        "outputId": "7523c089-7d3d-4877-f193-80a355995be6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaN values: 0\n",
            "Number of positive infinity values: 0\n",
            "Number of negative infinity values: 0\n",
            "Total number of infinity values: 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "1Of4P_9n1SysZl8BzuSoNqdsel4WqvoWb",
      "authorship_tag": "ABX9TyPsryeKupj6CmREMaM8G/UX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}