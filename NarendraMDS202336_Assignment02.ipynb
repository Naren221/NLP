{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naren221/NLP/blob/main/NarendraMDS202336_Assignment02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNDNEeKPBG3d"
      },
      "source": [
        "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      Assignment 02\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtwSDRBeBjH7"
      },
      "source": [
        "## **Narendra. C** <br> **MDS202336**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE01cn8bB7bN"
      },
      "source": [
        "## 1. Implementing Modified Verison of COALS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d-5bqYaCMMG"
      },
      "source": [
        "* The preprocessed corpus is stored in JSON files, where each file contains a collection of documents.\n",
        "* Each document in the JSON file is represented as a key-value pair:\n",
        "\n",
        "    * The key is the document name or ID, which serves as a unique identifier for the document.\n",
        "    * The value is the preprocessed text of that document, which consists of cleaned and tokenized words from the original text (e.g., lowercase, punctuation removed, stopwords removed).\n",
        "\n",
        "* This structure allows easy access to both the document identifiers and their\n",
        "corresponding preprocessed content, facilitating efficient data manipulation and analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BducjirYvm4S",
        "outputId": "7cc64d06-9bd6-4eda-e079-24bcf5f0c042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4DEBdK2FDAF",
        "outputId": "03226226-f28a-4289-fc05-7bbf520b7be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_yUyyvD5nAY"
      },
      "source": [
        "### Steps to Create a Co-occurrence Matrix\n",
        "\n",
        "1. **Identify Data Sources**\n",
        "   - Locate and list all relevant JSON files containing text data.\n",
        "\n",
        "2. **Extract Vocabulary**\n",
        "   - **Read Files:**\n",
        "     - Read and parse text data from each JSON file.\n",
        "   - **Build Vocabulary:**\n",
        "     - Extract unique tokens from each file and aggregate them into a global vocabulary.\n",
        "   - **Save Vocabulary:**\n",
        "     - Save the vocabulary to a file for future reference.\n",
        "\n",
        "3. **Initialize Co-occurrence Matrix**\n",
        "   - **Prepare Matrix:**\n",
        "     - Initialize a sparse matrix for storing co-occurrence counts based on the global vocabulary.\n",
        "\n",
        "4. **Process Each File**\n",
        "   - **Load Data:**\n",
        "     - Read and parse text data from each JSON file.\n",
        "   - **Update Matrix:**\n",
        "     - Convert tokens to matrix indices using the global vocabulary.\n",
        "     - Update the co-occurrence matrix based on token co-occurrences within a defined window size.\n",
        "\n",
        "5. **Finalize Matrix**\n",
        "   - **Ensure Symmetry:**\n",
        "     - Make the matrix symmetric by adding its transpose.\n",
        "   - **Convert Format:**\n",
        "     - Change the matrix to a more efficient storage format for further use (e.g., CSR format).\n",
        "\n",
        "6. **Save Results**\n",
        "   - **Store Matrix:**\n",
        "     - Save the matrix and vocabulary in a storage format for future use (e.g., HDF5 format).\n",
        "\n",
        "7. **Clean Up**\n",
        "   - Free up memory by deleting temporary data structures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apz3U2qF18cs",
        "outputId": "9f9157d6-2d64-45a1-e08f-236c257bbd16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting vocabulary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:17<00:00,  6.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary extracted and saved to: /content/drive/MyDrive/vocabulary.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_vocabulary(directory, output_path='/content/drive/MyDrive/vocabulary.txt'):\n",
        "    # Get the list of JSON files in the directory\n",
        "    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
        "\n",
        "    if not json_files:\n",
        "        raise ValueError(\"No JSON files found in the directory.\")\n",
        "\n",
        "    # Initialize a set to store the vocabulary\n",
        "    vocab = set()\n",
        "\n",
        "    # Process each JSON file with a progress bar\n",
        "    for json_file_name in tqdm(json_files, desc='Extracting vocabulary'):\n",
        "        file_path = os.path.join(directory, json_file_name)\n",
        "\n",
        "        # Read the JSON file\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Collect unique tokens from the current file\n",
        "        for document in data:\n",
        "            vocab.update(document)\n",
        "\n",
        "        # Free the data from memory\n",
        "        del data\n",
        "\n",
        "    # Save the vocabulary to a file\n",
        "    with open(output_path, 'w') as file:\n",
        "        for word in sorted(vocab):\n",
        "            file.write(f\"{word}\\n\")\n",
        "\n",
        "    print(f\"Vocabulary extracted and saved to: {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "directory = '/content/drive/MyDrive/NLP Assignments/preprocessed_files/'\n",
        "output_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "extract_vocabulary(directory, output_path=output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load vocabulary\n",
        "def load_vocabulary(vocab_path):\n",
        "    with open(vocab_path, 'r') as file:\n",
        "        vocab = [line.strip() for line in file]\n",
        "    return vocab\n",
        "\n",
        "# Function to process a single JSON file and build its co-occurrence matrix\n",
        "def process_single_file(file_path, vocab_index, window_size, output_path):\n",
        "    vocab_size = len(vocab_index)\n",
        "    cooccurrence_matrix = sp.lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
        "\n",
        "    # Read the JSON file\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Process each document in the current JSON file with a progress bar\n",
        "    with tqdm(total=len(data), desc=f'Processing {os.path.basename(file_path)}') as pbar:\n",
        "        for document in data:\n",
        "            token_indices = [vocab_index[token] for token in document if token in vocab_index]\n",
        "\n",
        "            for i, token_idx in enumerate(token_indices):\n",
        "                start_index = max(0, i - window_size)\n",
        "                end_index = min(len(token_indices), i + window_size + 1)\n",
        "\n",
        "                # Update co-occurrence for previous and future tokens without weights\n",
        "                for j in range(start_index, i):\n",
        "                    cooccurrence_matrix[token_idx, token_indices[j]] += 1\n",
        "\n",
        "                for j in range(i + 1, end_index):\n",
        "                    cooccurrence_matrix[token_idx, token_indices[j]] += 1\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Convert to CSR format for efficient storage\n",
        "    cooccurrence_matrix_csr = cooccurrence_matrix.tocsr()\n",
        "\n",
        "    # Store the matrix in a file\n",
        "    output_file = os.path.join(output_path, f'cooccurrence_matrix_{os.path.basename(file_path).split(\".\")[0]}.h5')\n",
        "    with h5py.File(output_file, 'w') as f:\n",
        "        f.create_dataset('data', data=cooccurrence_matrix_csr.data, compression='gzip')\n",
        "        f.create_dataset('indices', data=cooccurrence_matrix_csr.indices, compression='gzip')\n",
        "        f.create_dataset('indptr', data=cooccurrence_matrix_csr.indptr, compression='gzip')\n",
        "        f.attrs['shape'] = cooccurrence_matrix_csr.shape\n",
        "\n",
        "    # Free memory\n",
        "    del cooccurrence_matrix\n",
        "    del cooccurrence_matrix_csr\n",
        "    del data\n",
        "\n",
        "    print(f\"Co-occurrence matrix for {file_path} stored at: {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "def process_json_file(json_file_path, vocab_path, output_path, window_size=4):\n",
        "    vocab = load_vocabulary(vocab_path)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "    # Process and store co-occurrence matrix for the specified JSON file\n",
        "    process_single_file(json_file_path, vocab_index, window_size, output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__nrbo7tg_X_",
        "outputId": "991529d7-19b3-45a4-9836-bd777b11c61d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE5kFZUE2wmE",
        "outputId": "9d5c6507-eb0b-48b9-b821-2223d9a871d2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunk_5.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4711/4711 [23:00<00:00,  3.41it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_5.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_5.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunk_6.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4711/4711 [23:07<00:00,  3.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_6.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_6.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_7.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4711/4711 [22:10<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_7.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_7.h5\n"
          ]
        }
      ],
      "source": [
        "# file_number =  3 # Replace with the desired file number\n",
        "\n",
        "for file_number in [5, 6, 7]:\n",
        "  json_file_path = f'/content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_{file_number}.json'  # Path to your specific JSON file\n",
        "  directory = '/content/drive/MyDrive/NLP Assignments/preprocessed_files/'\n",
        "  vocab_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "  output_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/'\n",
        "\n",
        "  process_json_file(json_file_path, vocab_path, output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file_number in [8, 9, 10]:\n",
        "  json_file_path = f'/content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_{file_number}.json'  # Path to your specific JSON file\n",
        "  directory = '/content/drive/MyDrive/NLP Assignments/preprocessed_files/'\n",
        "  vocab_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "  output_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/'\n",
        "\n",
        "  process_json_file(json_file_path, vocab_path, output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY9iGrraKH8n",
        "outputId": "84a6323d-87fe-4189-980c-d6aa9a4cf22c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_8.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4711/4711 [21:41<00:00,  3.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_8.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_8.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_9.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4711/4711 [23:04<00:00,  3.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_9.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_9.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_10.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4711/4711 [24:27<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_10.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_10.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file_number in [11,12]:\n",
        "  json_file_path = f'/content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_{file_number}.json'  # Path to your specific JSON file\n",
        "  directory = '/content/drive/MyDrive/NLP Assignments/preprocessed_files/'\n",
        "  vocab_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "  output_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/'\n",
        "\n",
        "  process_json_file(json_file_path, vocab_path, output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neLUEkfc66h3",
        "outputId": "cecefd98-825f-4e0f-89fe-44825a9d3b41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_11.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4711/4711 [15:33<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_11.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_11.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_12.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4707/4707 [14:34<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix for /content/drive/MyDrive/NLP Assignments/preprocessed_files/chunk_12.json stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/cooccurrence_matrix_chunk_12.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I built a cooccurrence matrix for each chunk of the corpus.\n",
        "* This was done to manage tasks with limited memory.\n",
        "* Now i will combine these matrices to form a single cooccurrence matrix for the entire corpus."
      ],
      "metadata": {
        "id": "FbZWQpiXqP1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy.sparse as sp\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to incrementally combine matrices and write them to disk\n",
        "def combine_cooccurrence_matrices_incrementally(folder_path, vocab_size, output_file):\n",
        "    final_matrix = None\n",
        "\n",
        "    # Get all .h5 files in the folder\n",
        "    matrix_files = [f for f in os.listdir(folder_path) if f.startswith('cooccurrence_matrix') and f.endswith('.h5')]\n",
        "    if not matrix_files:\n",
        "        raise ValueError(\"No co-occurrence matrix files found in the folder.\")\n",
        "\n",
        "    # Combine each matrix into the final matrix incrementally\n",
        "    for file in tqdm(matrix_files, desc=\"Combining matrices\"):\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            data = f['data'][:]\n",
        "            indices = f['indices'][:]\n",
        "            indptr = f['indptr'][:]\n",
        "            shape = f.attrs['shape']\n",
        "\n",
        "        cooccurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "        # If final_matrix is None (i.e., first matrix), set it. Otherwise, add the new matrix\n",
        "        if final_matrix is None:\n",
        "            final_matrix = cooccurrence_matrix\n",
        "        else:\n",
        "            final_matrix = final_matrix + cooccurrence_matrix\n",
        "\n",
        "        # After processing each matrix, save intermediate results to avoid memory overload\n",
        "        _save_intermediate(final_matrix, output_file)\n",
        "        print(f\"\\nProcessed {file} and saved intermediate result.\")\n",
        "\n",
        "    # Return the final matrix\n",
        "    return final_matrix\n",
        "\n",
        "# Helper function to save intermediate results\n",
        "def _save_intermediate(matrix, output_file):\n",
        "    with h5py.File(output_file, 'w') as f:\n",
        "        f.create_dataset('data', data=matrix.data, compression='gzip')\n",
        "        f.create_dataset('indices', data=matrix.indices, compression='gzip')\n",
        "        f.create_dataset('indptr', data=matrix.indptr, compression='gzip')\n",
        "        f.attrs['shape'] = matrix.shape\n",
        "\n",
        "    print(f\"Intermediate result saved to: {output_file}\")\n",
        "\n",
        "# Function to store the final combined matrix\n",
        "def store_final_matrix(final_matrix, folder_path):\n",
        "    final_output_path = os.path.join(folder_path, 'final_cooccurrence_matrix.h5')\n",
        "    _save_intermediate(final_matrix, final_output_path)\n",
        "    print(f\"Final co-occurrence matrix stored at: {final_output_path} ðŸŽ‰\")\n"
      ],
      "metadata": {
        "id": "xNLr-FBGynH-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices'  # Folder where matrices are hanging out\n",
        "vocab_size = len(load_vocabulary('/content/drive/MyDrive/NLP Assignments/vocabulary.txt'))  # Load your vocab size\n",
        "output_file = '/content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5'  # Path to store the intermediate/final matrix\n",
        "\n",
        "# Call the function to combine matrices incrementally\n",
        "final_matrix = combine_cooccurrence_matrices_incrementally(folder_path, vocab_size, output_file)\n",
        "\n",
        "# After processing, you can store the final result\n",
        "store_final_matrix(final_matrix, '/content/drive/MyDrive/NLP Assignments/Assignment_02')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dvH6J1R1fnu",
        "outputId": "16313871-ada0-41b9-f0b2-e49e1884fa5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Combining matrices:   8%|â–Š         | 1/12 [00:12<02:12, 12.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_3.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  17%|â–ˆâ–‹        | 2/12 [00:31<02:41, 16.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_1.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:56<03:03, 20.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_2.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [01:26<03:12, 24.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_4.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [02:02<03:20, 28.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_5.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [02:44<03:17, 32.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_6.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [03:28<03:03, 36.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_7.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [04:16<02:41, 40.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_8.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [05:10<02:13, 44.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_9.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [06:09<01:38, 49.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_10.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCombining matrices:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [07:11<00:52, 52.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_11.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Combining matrices: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:14<00:00, 41.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/output_matrices/final_cooccurrence_matrix_intermediate.h5\n",
            "\n",
            "Processed cooccurrence_matrix_chunk_12.h5 and saved intermediate result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result saved to: /content/drive/MyDrive/NLP Assignments/Assignment_02/final_cooccurrence_matrix.h5\n",
            "Final co-occurrence matrix stored at: /content/drive/MyDrive/NLP Assignments/Assignment_02/final_cooccurrence_matrix.h5 ðŸŽ‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "def view_submatrix(file_path, row_start, col_start, size=5):\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        data = f['data'][:]\n",
        "        indices = f['indices'][:]\n",
        "        indptr = f['indptr'][:]\n",
        "        shape = f.attrs['shape']\n",
        "\n",
        "        # Create the CSR matrix from the stored data\n",
        "        cooccurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "        # Extract the submatrix\n",
        "        submatrix = cooccurrence_matrix[row_start:row_start + size, col_start:col_start + size].toarray()\n",
        "\n",
        "        print(\"10x10 Submatrix:\")\n",
        "        print(submatrix)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/final_cooccurrence_matrix.h5'\n",
        "view_submatrix(file_path, row_start=0, col_start=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMdnG_SnHsZR",
        "outputId": "f36a4542-b859-4233-e74b-2fe74cff39cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10x10 Submatrix:\n",
            "[[4.116e+03 4.900e+01 4.000e+00 1.000e+00 0.000e+00]\n",
            " [4.900e+01 1.600e+02 5.000e+00 2.000e+00 0.000e+00]\n",
            " [4.000e+00 5.000e+00 3.400e+01 1.000e+00 1.000e+00]\n",
            " [1.000e+00 2.000e+00 1.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "vocab = load_vocabulary(vocab_path)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLCxnbmVgmY1",
        "outputId": "a801e637-1857-48dd-a75c-126a1e77f3c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1470595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYWk3GJVmF6p",
        "outputId": "151a6c7c-8113-4949-8373-9545b7095efb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now i will try to reduce the size of the vocabulary to 7000."
      ],
      "metadata": {
        "id": "liWp8cHGnnQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I am reducing the vocabulary size by **filtering out infrequent words**, specifically those that have **low co-occurrence frequencies**. Additionally, stop words will be removed. This optimization aims to enhance the co-occurrence matrix and improve overall efficiency.\n"
      ],
      "metadata": {
        "id": "01Z3tBjfwp6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reason for Reducing Vocabulary Size This Way\n",
        "\n",
        "* Reducing vocabulary size based on **co-occurrence frequency** helps prioritize words that frequently appear together, capturing more meaningful word relationships.\n",
        "* Filtering out terms with low co-occurrence reduces noise and leads to a more efficient, less sparse co-occurrence matrix, improving computational performance.\n",
        "* This method emphasizes relationships between words rather than individual token frequency, offering a more insightful analysis of the text.\n",
        "* The approach focuses on selecting the **top 7000** words based on co-occurrence frequency, ensuring that significant terms are retained while less informative ones are removed.\n"
      ],
      "metadata": {
        "id": "56WFQalZkoDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk.probability import FreqDist\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize frequency distribution\n",
        "def process_chunk(chunk_file):\n",
        "    freq_dist = FreqDist()\n",
        "\n",
        "    with open(chunk_file, 'r') as file:\n",
        "        token_lists = json.load(file)\n",
        "\n",
        "    for token_list in token_lists:\n",
        "        freq_dist.update(token_list)\n",
        "\n",
        "    return freq_dist\n",
        "\n",
        "# Path to the folder where token chunks are stored\n",
        "chunks_folder = '/content/drive/MyDrive/NLP Assignments/preprocessed_files'\n",
        "chunk_files = [f'{chunks_folder}/chunk_{i}.json' for i in range(1, 13)]  # Assuming 12 chunks\n",
        "\n",
        "# Process chunks sequentially\n",
        "final_freq_dist = FreqDist()\n",
        "for chunk_file in tqdm(chunk_files, desc=\"Processing chunks\"):\n",
        "    freq_dist = process_chunk(chunk_file)\n",
        "    final_freq_dist.update(freq_dist)\n",
        "\n",
        "# Filter the vocabulary based on a frequency threshold\n",
        "frequency_threshold = 2210  # Adjust this threshold\n",
        "vocab = [word for word, freq in final_freq_dist.items() if freq > frequency_threshold]\n",
        "\n",
        "# Save the new vocabulary\n",
        "vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_vocab.txt'\n",
        "with open(vocab_path, 'w') as f:\n",
        "    f.write(\"\\n\".join(vocab))\n",
        "\n",
        "print(f\"Vocabulary size after reduction: {len(vocab)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vOQSTWqS70K",
        "outputId": "6e7c05c3-c275-44d7-a2c6-dc66c2d820f6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [02:19<00:00, 11.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size after reduction: 7161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_vocab.txt'\n",
        "vocab = load_vocabulary(vocab_path)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uibJlfKGihqn",
        "outputId": "27f8192f-f74e-4b6a-e4da-3c89d6593463"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. I noticed some letters present in the vocab which can be considered as stop words ... for eg there are alphabets like `e` and all which are just letters. I will remove those from the vocabulary."
      ],
      "metadata": {
        "id": "aQNrqEdmjR9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of stop words\n",
        "stop_words = [\n",
        "    'given', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n",
        "    'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
        "    'u', 'v', 'w', 'x', 'y', 'z'\n",
        "]\n",
        "\n",
        "# Removing stop words from the vocabulary\n",
        "filtered_vocab = [word for word in vocab if word not in stop_words]\n",
        "\n",
        "print(len(filtered_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvN1T7RcjRsS",
        "outputId": "f1fbe38a-700a-4597-c7e2-a0865a9469b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Since the corpus is made from research papers things like `et.al.` will be present. we can remove those as well."
      ],
      "metadata": {
        "id": "8vD8YI-Pjzph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of terms to remove\n",
        "terms_to_remove = ['et', 'al', 'ie', 'eg', 'etc', 'aka', 'viz', 'cf', 'nb', 'pm', 'vs']\n",
        "\n",
        "# Remove the terms from the vocabulary\n",
        "filtered_vocab = [word for word in filtered_vocab if word not in terms_to_remove]\n",
        "\n",
        "len(filtered_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoSg3Q9-jRow",
        "outputId": "c3645367-025f-4a76-a812-0905a5c75c7e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7134"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now i will store this filtered vocab."
      ],
      "metadata": {
        "id": "WGsDnGRIlcEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the filtered vocabulary\n",
        "filtered_vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_vocab.txt'\n",
        "with open(filtered_vocab_path, 'w') as f:\n",
        "    f.write(\"\\n\".join(filtered_vocab))"
      ],
      "metadata": {
        "id": "AAyz45mdlg90"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now i will reduce the size of the cooccurrence matrix as well."
      ],
      "metadata": {
        "id": "gAEvspD9lFSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the original co-occurrence matrix (sparse format)\n",
        "co_occurrence_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/final_cooccurrence_matrix.h5'\n",
        "with h5py.File(co_occurrence_path, 'r') as f:\n",
        "    data = f['data'][:]\n",
        "    indices = f['indices'][:]\n",
        "    indptr = f['indptr'][:]\n",
        "    shape = f.attrs['shape']\n",
        "\n",
        "# Rebuild the sparse co-occurrence matrix (CSR format)\n",
        "co_occurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "# Load the filtered vocabulary\n",
        "filtered_vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_vocab.txt'\n",
        "with open(filtered_vocab_path, 'r') as file:\n",
        "    filtered_vocab = [line.strip() for line in file]\n",
        "\n",
        "# Create a mapping from vocabulary words to their indices\n",
        "vocab_index = {word: i for i, word in enumerate(filtered_vocab)}\n",
        "\n",
        "# Create a new co-occurrence matrix based on the filtered vocabulary\n",
        "filtered_matrix = sp.lil_matrix((len(filtered_vocab), len(filtered_vocab)), dtype=np.float32)\n",
        "\n",
        "# Populate the filtered co-occurrence matrix based on the original matrix with progress bar\n",
        "for word, new_index in tqdm(vocab_index.items(), desc=\"Building filtered co-occurrence matrix\", total=len(vocab_index)):\n",
        "    original_index = vocab_index.get(word)  # Get the index of the word in the original matrix\n",
        "\n",
        "    if original_index is not None:\n",
        "        # Copy relevant row and column from the original matrix\n",
        "        filtered_matrix[new_index, :] = co_occurrence_matrix[original_index, :].toarray()[0, list(vocab_index.values())]\n",
        "        filtered_matrix[:, new_index] = co_occurrence_matrix[:, original_index].toarray()[list(vocab_index.values()), 0]\n",
        "\n",
        "# Convert the matrix to CSR for efficiency\n",
        "filtered_matrix_csr = filtered_matrix.tocsr()\n",
        "\n",
        "# Save the new filtered co-occurrence matrix\n",
        "filtered_co_occurrence_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_cooccurrence_matrix.h5'\n",
        "with h5py.File(filtered_co_occurrence_path, 'w') as f:\n",
        "    f.create_dataset('data', data=filtered_matrix_csr.data, compression='gzip')\n",
        "    f.create_dataset('indices', data=filtered_matrix_csr.indices, compression='gzip')\n",
        "    f.create_dataset('indptr', data=filtered_matrix_csr.indptr, compression='gzip')\n",
        "    f.attrs['shape'] = filtered_matrix_csr.shape\n",
        "\n",
        "print(f\"Filtered co-occurrence matrix saved at: {filtered_co_occurrence_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Waoo295lDpl",
        "outputId": "af620340-e23b-4ffa-ad5b-8cf1ebb2c5f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building filtered co-occurrence matrix: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7134/7134 [31:37<00:00,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered co-occurrence matrix saved at: /content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_cooccurrence_matrix.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_cooccurrence_matrix.h5'\n",
        "view_submatrix(file_path, row_start=6000, col_start=5000, size = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3al94LOjRlI",
        "outputId": "fade99d5-efac-49c4-8616-bb6990847ed2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10x10 Submatrix:\n",
            "[[  8.   4.   2.   5.   5.   6.  10.   5.   7.   9.]\n",
            " [  7.  27.  11.  24.  18.  20.  13.  37.  17.  19.]\n",
            " [ 21.   2.  35.   3.   0. 146.   3.  10.   3.  24.]\n",
            " [  0.  37.   2.   8. 152.   1.  15.   5.   8.   3.]\n",
            " [  7.   0.   1.   1.   7.   3.   1.   0.   0.  12.]\n",
            " [ 21.  27.   4.  15.   8.   5.  16.   7.   3.  25.]\n",
            " [  6.  11.  10.  13.  31.   1.  71.  15.  24.   6.]\n",
            " [  3.   2.  13.   6.   7.  15.   4.   0.   9.  62.]\n",
            " [  3.   1.  30.   4.  15.   3.   7.   5.   3.   4.]\n",
            " [ 10.  49.  14.   9.  19.  13.  25.  24.   3.   4.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Load the filtered co-occurrence matrix\n",
        "filtered_co_occurrence_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_cooccurrence_matrix.h5'\n",
        "\n",
        "with h5py.File(filtered_co_occurrence_path, 'r') as f:\n",
        "    data = f['data'][:]\n",
        "    indices = f['indices'][:]\n",
        "    indptr = f['indptr'][:]\n",
        "    shape = f.attrs['shape']\n",
        "\n",
        "# Rebuild the sparse co-occurrence matrix (CSR format)\n",
        "filtered_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "# Calculate the total number of elements and the number of non-zero elements\n",
        "total_elements = filtered_matrix.shape[0] * filtered_matrix.shape[1]\n",
        "non_zero_elements = filtered_matrix.nnz\n",
        "\n",
        "# Calculate the number of zeros and the percentage of zeros\n",
        "number_of_zeros = total_elements - non_zero_elements\n",
        "percentage_of_zeros = (number_of_zeros / total_elements) * 100\n",
        "\n",
        "print(f\"Total elements: {total_elements}\")\n",
        "print(f\"Non-zero elements: {non_zero_elements}\")\n",
        "print(f\"Percentage of zeros: {percentage_of_zeros:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-hIjtUc5g7t",
        "outputId": "6dbaf96f-6835-4ee3-bd8a-5de5dc1faa85"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total elements: 49000000\n",
            "Non-zero elements: 30923113\n",
            "Percentage of zeros: 36.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the filtered vocabulary (7000+ words)\n",
        "def load_vocabulary(vocab_path):\n",
        "    with open(vocab_path, 'r') as file:\n",
        "        vocab = [line.strip() for line in file]\n",
        "    return vocab\n",
        "\n",
        "# Function to process a single JSON chunk file and build the co-occurrence matrix\n",
        "def process_single_file(file_path, vocab_index, window_size, output_path):\n",
        "    vocab_size = len(vocab_index)\n",
        "    cooccurrence_matrix = sp.lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
        "\n",
        "    # Read the JSON file chunk\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Process each document in the current JSON file chunk\n",
        "    with tqdm(total=len(data), desc=f'Processing {os.path.basename(file_path)}') as pbar:\n",
        "        for document in data:\n",
        "            # Filter and index the tokens based on the selected vocabulary\n",
        "            token_indices = [vocab_index[token] for token in document if token in vocab_index]\n",
        "\n",
        "            # Update the co-occurrence matrix\n",
        "            for i, token_idx in enumerate(token_indices):\n",
        "                start_index = max(0, i - window_size)\n",
        "                end_index = min(len(token_indices), i + window_size + 1)\n",
        "\n",
        "                # Update co-occurrence for previous and future tokens in the window\n",
        "                for j in range(start_index, i):\n",
        "                    cooccurrence_matrix[token_idx, token_indices[j]] += 1\n",
        "                for j in range(i + 1, end_index):\n",
        "                    cooccurrence_matrix[token_idx, token_indices[j]] += 1\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Convert to CSR format for efficient storage\n",
        "    cooccurrence_matrix_csr = cooccurrence_matrix.tocsr()\n",
        "\n",
        "    # Store the co-occurrence matrix in HDF5 format\n",
        "    output_file = os.path.join(output_path, f'cooccurrence_matrix_{os.path.basename(file_path).split(\".\")[0]}.h5')\n",
        "    with h5py.File(output_file, 'w') as f:\n",
        "        f.create_dataset('data', data=cooccurrence_matrix_csr.data, compression='gzip')\n",
        "        f.create_dataset('indices', data=cooccurrence_matrix_csr.indices, compression='gzip')\n",
        "        f.create_dataset('indptr', data=cooccurrence_matrix_csr.indptr, compression='gzip')\n",
        "        f.attrs['shape'] = cooccurrence_matrix_csr.shape\n",
        "\n",
        "    # Clear memory\n",
        "    del cooccurrence_matrix\n",
        "    del cooccurrence_matrix_csr\n",
        "    del data\n",
        "\n",
        "    print(f\"Co-occurrence matrix for {file_path} saved at: {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "# Function to iterate over all JSON files and build co-occurrence matrices\n",
        "def process_json_files(json_files_dir, vocab_path, output_path, window_size=4):\n",
        "    vocab = load_vocabulary(vocab_path)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "    # Iterate over all files in the JSON directory\n",
        "    for json_file in os.listdir(json_files_dir):\n",
        "        if json_file.endswith('.json'):\n",
        "            json_file_path = os.path.join(json_files_dir, json_file)\n",
        "            process_single_file(json_file_path, vocab_index, window_size, output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJQ4qCraIu_F",
        "outputId": "946762ed-9d75-408e-b646-d62720aa951c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Path to filtered vocabulary\n",
        "vocab_path = '/content/drive/MyDrive/NLP Assignments/Assignment_02/filtered_vocab.txt'\n",
        "\n",
        "# Directory containing JSON corpus chunks\n",
        "json_files_dir = '/content/drive/MyDrive/NLP Assignments/preprocessed_files'\n",
        "\n",
        "# Output directory for co-occurrence matrix files\n",
        "output_path = '/content/drive/MyDrive/NLP Assignments/cooccurrence_matrices/'\n",
        "\n",
        "# Process all JSON files and build co-occurrence matrices with a window size of 4\n",
        "process_json_files(json_files_dir, vocab_path, output_path, window_size=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "47a-5AvZIyIh",
        "outputId": "30d9f324-4a74-48ad-a859-63e1b77d235d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunk_1.json:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2843/4711 [10:40<07:01,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-2e2740f73e3a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Process all JSON files and build co-occurrence matrices with a window size of 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprocess_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_files_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-10e6a92a9f18>\u001b[0m in \u001b[0;36mprocess_json_files\u001b[0;34m(json_files_dir, vocab_path, output_path, window_size)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mjson_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_files_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mprocess_single_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-10e6a92a9f18>\u001b[0m in \u001b[0;36mprocess_single_file\u001b[0;34m(file_path, vocab_index, window_size, output_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mcooccurrence_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mcooccurrence_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_lil.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 isinstance(key[1], INT_TYPES)):\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# lil_get1 handles validation for us.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_intXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Everything else takes the normal path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndexMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_lil.py\u001b[0m in \u001b[0;36m_get_intXint\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_intXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         v = _csparsetools.lil_get1(self.shape[0], self.shape[1], self.rows,\n\u001b[0m\u001b[1;32m    165\u001b[0m                                    self.data, row, col)\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import h5py\n",
        "\n",
        "# Step 1: Load the original co-occurrence matrix from the HDF5 file\n",
        "def load_cooccurrence_matrix(hdf5_file):\n",
        "    with h5py.File(hdf5_file, 'r') as f:\n",
        "        data = f['data'][:]\n",
        "        indices = f['indices'][:]\n",
        "        indptr = f['indptr'][:]\n",
        "        shape = f.attrs['shape']\n",
        "\n",
        "        # Reconstruct sparse matrix\n",
        "        cooccurrence_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)\n",
        "\n",
        "    return cooccurrence_matrix\n",
        "\n",
        "# Step 2: Load the original vocabulary (for 1.4 million words)\n",
        "def load_vocabulary(vocab_file):\n",
        "    with open(vocab_file, 'r') as file:\n",
        "        vocab = [line.strip() for line in file]\n",
        "    return vocab\n",
        "\n",
        "# Step 3: Sum co-occurrence frequencies for each word\n",
        "def get_cooccurrence_sums(cooccurrence_matrix):\n",
        "    # Sum along rows to get the total co-occurrence frequency for each word\n",
        "    word_cooccurrence_sums = np.array(cooccurrence_matrix.sum(axis=1)).flatten()\n",
        "    return word_cooccurrence_sums\n",
        "\n",
        "# Step 4: Rank words by total co-occurrence frequency and get top N words\n",
        "def select_top_words(cooccurrence_sums, vocab_size):\n",
        "    # Get indices of top N words based on co-occurrence sums\n",
        "    top_word_indices = np.argsort(cooccurrence_sums)[-vocab_size:]\n",
        "    return top_word_indices\n",
        "\n",
        "# Step 5: Filter the co-occurrence matrix to keep only the top words\n",
        "def filter_cooccurrence_matrix(cooccurrence_matrix, top_word_indices):\n",
        "    # Extract rows and columns corresponding to top_word_indices\n",
        "    reduced_matrix = cooccurrence_matrix[top_word_indices, :][:, top_word_indices]\n",
        "    return reduced_matrix\n",
        "\n",
        "# Step 6: Save the reduced co-occurrence matrix and reduced vocabulary\n",
        "def save_reduced_matrix_and_vocab(reduced_matrix, reduced_vocab, output_matrix_file, output_vocab_file):\n",
        "    # Save reduced co-occurrence matrix to HDF5\n",
        "    with h5py.File(output_matrix_file, 'w') as f:\n",
        "        f.create_dataset('data', data=reduced_matrix.data, compression='gzip')\n",
        "        f.create_dataset('indices', data=reduced_matrix.indices, compression='gzip')\n",
        "        f.create_dataset('indptr', data=reduced_matrix.indptr, compression='gzip')\n",
        "        f.attrs['shape'] = reduced_matrix.shape\n",
        "\n",
        "    # Save reduced vocabulary to a text file\n",
        "    with open(output_vocab_file, 'w') as f:\n",
        "        for word in reduced_vocab:\n",
        "            f.write(f\"{word}\\n\")\n",
        "\n",
        "# Main function to reduce co-occurrence matrix and vocabulary by co-occurrence frequency\n",
        "def reduce_matrix_and_vocab(hdf5_file, vocab_file, output_matrix_file, output_vocab_file, vocab_size):\n",
        "    # Load the original co-occurrence matrix\n",
        "    cooccurrence_matrix = load_cooccurrence_matrix(hdf5_file)\n",
        "\n",
        "    # Load the original vocabulary\n",
        "    vocab = load_vocabulary(vocab_file)\n",
        "\n",
        "    # Get co-occurrence sums for each word\n",
        "    cooccurrence_sums = get_cooccurrence_sums(cooccurrence_matrix)\n",
        "\n",
        "    # Select top words based on co-occurrence frequency\n",
        "    top_word_indices = select_top_words(cooccurrence_sums, vocab_size)\n",
        "\n",
        "    # Filter the matrix to keep only the top words\n",
        "    reduced_matrix = filter_cooccurrence_matrix(cooccurrence_matrix, top_word_indices)\n",
        "\n",
        "    # Get the reduced vocabulary\n",
        "    reduced_vocab = [vocab[i] for i in top_word_indices]\n",
        "\n",
        "    # Save the reduced matrix and vocabulary\n",
        "    save_reduced_matrix_and_vocab(reduced_matrix, reduced_vocab, output_matrix_file, output_vocab_file)\n",
        "\n",
        "# Paths to original matrix, vocab, and reduced output files\n",
        "original_matrix_file = '/content/drive/MyDrive/NLP Assignments/Assignment_02/final_cooccurrence_matrix.h5'\n",
        "original_vocab_file = '/content/drive/MyDrive/NLP Assignments/vocabulary.txt'\n",
        "reduced_matrix_file = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_cooccurrence_matrix.h5'\n",
        "reduced_vocab_file = '/content/drive/MyDrive/NLP Assignments/Assignment_02/reduced_vocabulary_2.txt'\n",
        "vocab_size = 7000  # Number of top words to keep\n",
        "\n",
        "# Run the reduction process\n",
        "reduce_matrix_and_vocab(original_matrix_file, original_vocab_file, reduced_matrix_file, reduced_vocab_file, vocab_size)\n"
      ],
      "metadata": {
        "id": "dVhmxD8TLk7W"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Co-occurrence Matrix\n",
        "\n",
        "The co-occurrence matrix used in this assignment is too large to be displayed here directly. You can download or view the matrix using the following link:\n",
        "\n",
        "[Download Co-occurrence Matrix](https://drive.google.com/file/d/your-file-id/view?usp=sharing)\n",
        "\n",
        "_Note: This matrix is stored in HDF5 format due to its large size._\n"
      ],
      "metadata": {
        "id": "nyfznYHH0dZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing the entries using proabilities $\\frac {P_{ik}} {P_{jk}}$\n",
        "\n",
        "$X$      -> Co-occurrence Matrix      <br>\n",
        "$X_{ij}$ -> Number of times word j occurs in the context of word i. <br>\n",
        "$X_i = \\sum_k X_{ik}$ -> Number of any word appears in the context of word i.<br>\n",
        "$P_{ij} = P(j|i) = \\frac {X_{ij} } {X_i}$ <br>\n",
        "This gives the probability that the word j occurs in the contex of the word i.\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\frac {P_{ik}} {P_{jk}} = {P(k|i)} {P(k|j)} $ <br>\n",
        "\n",
        "<br>\n",
        "The ratio  $ \\frac {P(k|i)} {P(k|j)}$\n",
        "helps normalize co-occurrence counts by considering the probability of context words with respect to different target words. This normalization emphasizes the relative strength of association between target words, making it easier to distinguish meaningful patterns and relationships.\n",
        "\n",
        "### Choosing the word for k\n",
        "\n",
        "**Value of k:** High-Frequency Words (e.g., \"the\", \"and\", \"of\")\n",
        "\n",
        "**Reason for Choosing This:**\n",
        "\n",
        "In co-occurrence matrix normalization, selecting high-frequency words as the reference `k` helps address the issue of frequency bias. High-frequency words, being common across various contexts, serve as a stable baseline for comparison. By normalizing co-occurrence counts using these high-frequency words, we adjust for their overwhelming presence and mitigate their disproportionate influence. The ratio of co-occurrence probabilities $ \\frac{P(k | i)}{P(k | j)} $ helps us understand the relative strength of the association between the target words `i` and `j` compared to their association with a high-frequency word `k`. This normalization process emphasizes the distinctive relationships between `i` and `j` by neutralizing the impact of common words that might otherwise dominate the matrix. Thus, using high-frequency words ensures that the co-occurrence matrix reflects more meaningful and specific patterns rather than being skewed by the frequent occurrence of generic words.\n",
        "\n"
      ],
      "metadata": {
        "id": "5U_eC53NyACN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps for Normalizing Co-occurrence Matrix\n",
        "\n",
        "1. **Find the Most Frequent Word**:\n",
        "   Start by summing each row (or column) of the co-occurrence matrix to get the total occurrences of each word across all contexts. The word with the highest total is identified as the most frequent word.\n",
        "\n",
        "2. **Normalize the Co-occurrence Matrix**:\n",
        "   Using the most frequent word $ k $, normalize the co-occurrence matrix. The normalization is done by calculating the ratio $ \\frac{P(k | i)}{P(k | j)} $, where:\n",
        "   - $ P(k | i) $ is the probability of word $ k $ occurring in the context of word $ i $.\n",
        "   - $ P(k | j) $ is the probability of word $ k $ occurring in the context of word $ j $.\n",
        "   - This step adjusts the co-occurrence counts by considering the probability of context words with respect to different target words.\n",
        "\n",
        "3. **Remove the Most Frequent Word**:\n",
        "   Once the normalization is done, remove the row and column corresponding to the most frequent word from the co-occurrence matrix. Also, remove the most frequent word from the vocabulary list, so it no longer influences further analysis.\n"
      ],
      "metadata": {
        "id": "KNy6AbuYzMMx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ip7WK3mkjRhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ccPsb1SAjRd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7tBrbqO8jRat"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "1Of4P_9n1SysZl8BzuSoNqdsel4WqvoWb",
      "authorship_tag": "ABX9TyPpo9WGQ/31ZIUi2sb3LKnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}